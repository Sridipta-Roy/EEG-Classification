{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4sU8G6RgOggg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sU8G6RgOggg",
        "outputId": "e9ef6513-4619-45bb-8452-2332c679c3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'EEG-Classification'...\n",
            "remote: Enumerating objects: 594, done.\u001b[K\n",
            "remote: Total 594 (delta 0), reused 0 (delta 0), pack-reused 594 (from 2)\u001b[K\n",
            "Receiving objects: 100% (594/594), 141.83 MiB | 15.70 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "Updating files: 100% (666/666), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/Sridipta-Roy/EEG-Classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae95d60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5ae95d60",
        "outputId": "55c1e18c-3ed1-4783-fad7-e45286df2953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2.0.0,>=1.21.0 (from -r /content/EEG-Classification/requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 4)) (1.16.3)\n",
            "Collecting mne<1.11.0,>=1.0.0 (from -r /content/EEG-Classification/requirements.txt (line 7))\n",
            "  Downloading mne-1.10.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 8)) (1.9.0)\n",
            "Collecting pyedflib>=0.1.30 (from -r /content/EEG-Classification/requirements.txt (line 9))\n",
            "  Downloading pyedflib-0.1.42-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 15)) (3.10.0)\n",
            "Requirement already satisfied: seaborn<1.0.0,>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 16)) (0.13.2)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 17)) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 20)) (1.6.1)\n",
            "Requirement already satisfied: xgboost>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 21)) (3.1.1)\n",
            "Requirement already satisfied: lightgbm>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 22)) (4.6.0)\n",
            "Requirement already satisfied: imbalanced-learn>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 23)) (0.14.0)\n",
            "Requirement already satisfied: tensorflow<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 26)) (2.19.0)\n",
            "Requirement already satisfied: keras>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 27)) (3.10.0)\n",
            "Requirement already satisfied: tsfresh>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 31)) (0.21.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 34)) (0.14.5)\n",
            "Requirement already satisfied: tqdm>=4.62.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 37)) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 38)) (1.5.2)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 39)) (6.0.3)\n",
            "Collecting jupyter>=1.0.0 (from -r /content/EEG-Classification/requirements.txt (line 42))\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: ipykernel>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 43)) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 44)) (7.7.1)\n",
            "Requirement already satisfied: notebook>=6.4.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/EEG-Classification/requirements.txt (line 45)) (6.5.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.3.0->-r /content/EEG-Classification/requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.3.0->-r /content/EEG-Classification/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.3.0->-r /content/EEG-Classification/requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne<1.11.0,>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne<1.11.0,>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne<1.11.0,>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne<1.11.0,>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 7)) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne<1.11.0,>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 7)) (1.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.4.0->-r /content/EEG-Classification/requirements.txt (line 15)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.4.0->-r /content/EEG-Classification/requirements.txt (line 15)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.4.0->-r /content/EEG-Classification/requirements.txt (line 15)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.4.0->-r /content/EEG-Classification/requirements.txt (line 15)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.4.0->-r /content/EEG-Classification/requirements.txt (line 15)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.4.0->-r /content/EEG-Classification/requirements.txt (line 15)) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly<6.0.0,>=5.0.0->-r /content/EEG-Classification/requirements.txt (line 17)) (8.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 20)) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost>=1.5.0->-r /content/EEG-Classification/requirements.txt (line 21)) (2.27.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (2.19.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (0.5.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 27)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 27)) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 27)) (0.17.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from tsfresh>=0.19.0->-r /content/EEG-Classification/requirements.txt (line 31)) (1.0.2)\n",
            "Requirement already satisfied: stumpy>=1.7.2 in /usr/local/lib/python3.12/dist-packages (from tsfresh>=0.19.0->-r /content/EEG-Classification/requirements.txt (line 31)) (1.13.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from tsfresh>=0.19.0->-r /content/EEG-Classification/requirements.txt (line 31)) (3.1.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (from jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (7.16.6)\n",
            "Collecting jupyterlab (from jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42))\n",
            "  Downloading jupyterlab-4.4.10-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (6.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.6.0->-r /content/EEG-Classification/requirements.txt (line 44)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.6.0->-r /content/EEG-Classification/requirements.txt (line 44)) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets>=7.6.0->-r /content/EEG-Classification/requirements.txt (line 44)) (3.0.15)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (25.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (5.9.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (1.3.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (0.45.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (4.9.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.6.1->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (4.5.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (4.25.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (2025.10.5)\n",
            "Requirement already satisfied: numba>=0.57.1 in /usr/local/lib/python3.12/dist-packages (from stumpy>=1.7.2->tsfresh>=0.19.0->-r /content/EEG-Classification/requirements.txt (line 31)) (0.60.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3.0.0,>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 26)) (3.1.3)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (25.1.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42))\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42))\n",
            "  Downloading jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (2.14.0)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42))\n",
            "  Downloading jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 27)) (4.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (0.8.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (0.28.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.9.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42))\n",
            "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.8.0->-r /content/EEG-Classification/requirements.txt (line 27)) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.57.1->stumpy>=1.7.2->tsfresh>=0.19.0->-r /content/EEG-Classification/requirements.txt (line 31)) (0.43.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.0.0->-r /content/EEG-Classification/requirements.txt (line 43)) (0.2.14)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (2.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.4.0->-r /content/EEG-Classification/requirements.txt (line 45)) (2.23)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r /content/EEG-Classification/requirements.txt (line 42)) (1.4.0)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mne-1.10.2-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyedflib-0.1.42-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jupyterlab-4.4.10-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: numpy, json5, jedi, async-lru, pyedflib, mne, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed async-lru-2.0.5 jedi-0.19.2 json5-0.12.1 jupyter-1.1.1 jupyter-lsp-2.3.0 jupyterlab-4.4.10 jupyterlab-server-2.28.0 mne-1.10.2 numpy-1.26.4 pyedflib-0.1.42\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "8b17d2fbdfc34be5acbbc0407bc734b7",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %pip install -r /content/EEG-Classification/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc2fce30",
      "metadata": {
        "id": "cc2fce30"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                            accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Custom modules\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "#sys.path.append('/content/EEG-Classification/src')\n",
        "from models import TraditionalMLModels, DeepLearningModels, get_callbacks\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fce90d89",
      "metadata": {
        "id": "fce90d89"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a55df30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a55df30",
        "outputId": "7e216903-6345-4691-fc35-2918721d2410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "NyxWKjPYQB7g",
      "metadata": {
        "id": "NyxWKjPYQB7g"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = Path('../data/processed/bonn')\n",
        "FEATURES_DIR = DATA_DIR / 'features'\n",
        "MODELS_DIR = Path('../results/models')\n",
        "FIGURES_DIR = Path('../results/figures')\n",
        "\n",
        "# DATA_DIR = Path('/content/EEG-Classification/data/processed/bonn')\n",
        "# FEATURES_DIR = DATA_DIR / 'features'\n",
        "# MODELS_DIR = Path('/content/EEG-Classification/results/models')\n",
        "# FIGURES_DIR = Path('/content/EEG-Classification/results/figures')\n",
        "\n",
        "# Create directories\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1hOu0iyJQ3_O",
      "metadata": {
        "id": "1hOu0iyJQ3_O"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    'test_size': 0.15,\n",
        "    'val_size': 0.15,\n",
        "    'random_state': 42,\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'patience': 10,\n",
        "    'learning_rate': 0.001,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "tWaxZNqARHSE",
      "metadata": {
        "id": "tWaxZNqARHSE"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "TC6li9B8RJ7S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC6li9B8RJ7S",
        "outputId": "8782329b-5eb6-4f46-c7ec-d89bca880708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "1. Loading engineered features for traditional ML...\n",
            " Training features: (5600, 74)\n",
            " Validation features: (1200, 74)\n",
            " Test features: (1200, 74)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load feature-based data for traditional ML\n",
        "print(\"\\n1. Loading engineered features for traditional ML...\")\n",
        "X_train_features = np.load(FEATURES_DIR / 'X_train_features.npy')\n",
        "y_train = np.load(FEATURES_DIR / 'y_train.npy')\n",
        "X_val_features = np.load(FEATURES_DIR / 'X_val_features.npy')\n",
        "y_val = np.load(FEATURES_DIR / 'y_val.npy')\n",
        "X_test_features = np.load(FEATURES_DIR / 'X_test_features.npy')\n",
        "y_test = np.load(FEATURES_DIR / 'y_test.npy')\n",
        "\n",
        "print(f\" Training features: {X_train_features.shape}\")\n",
        "print(f\" Validation features: {X_val_features.shape}\")\n",
        "print(f\" Test features: {X_test_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6-TIKqvGRUNc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-TIKqvGRUNc",
        "outputId": "1fbf79d8-fc3c-4866-9d51-9be963a4d2be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. Loading raw preprocessed signals for deep learning...\n",
            " Training signals: (5600, 694)\n",
            " Validation signals: (1200, 694)\n",
            " Test signals: (1200, 694)\n"
          ]
        }
      ],
      "source": [
        "# Load raw signals for deep learning\n",
        "print(\"\\n2. Loading raw preprocessed signals for deep learning...\")\n",
        "splits_dir = DATA_DIR / 'splits'\n",
        "X_train_raw = np.load(splits_dir / 'X_train.npy')\n",
        "X_val_raw = np.load(splits_dir / 'X_val.npy')\n",
        "X_test_raw = np.load(splits_dir / 'X_test.npy')\n",
        "\n",
        "print(f\" Training signals: {X_train_raw.shape}\")\n",
        "print(f\" Validation signals: {X_val_raw.shape}\")\n",
        "print(f\" Test signals: {X_test_raw.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "zPaRRFPQRxh6",
      "metadata": {
        "id": "zPaRRFPQRxh6"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "#   DATA PREPARATION\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Jvci7NJ4R0tS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvci7NJ4R0tS",
        "outputId": "96f4f2cc-a809-41d1-93d8-bbb5aa25e21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Standardizing features for traditional ML...\n",
            "Features standardized (mean≈0, std≈1)\n",
            "Training: mean=0.000000, std=1.000000\n"
          ]
        }
      ],
      "source": [
        "# Standardize features for traditional ML\n",
        "print(\"\\nStandardizing features for traditional ML...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "X_val_scaled = scaler.transform(X_val_features)\n",
        "X_test_scaled = scaler.transform(X_test_features)\n",
        "\n",
        "print(f\"Features standardized (mean≈0, std≈1)\")\n",
        "print(f\"Training: mean={X_train_scaled.mean():.6f}, std={X_train_scaled.std():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "umv_PpXQSFH7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umv_PpXQSFH7",
        "outputId": "614c7aa1-df13-49e3-cf5a-cb56e10cfc69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved scaler to /content/EEG-Classification/results/models/feature_scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "# Save scaler for future use\n",
        "joblib.dump(scaler, MODELS_DIR / 'feature_scaler.pkl')\n",
        "print(f\"Saved scaler to {MODELS_DIR}/feature_scaler.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RG-iriWsSYI2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG-iriWsSYI2",
        "outputId": "6c7ad6d7-2696-479d-aee2-de0541654a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reshaping signals for deep learning...\n",
            "Reshaped for DL: (5600, 694, 1)\n",
            "Input shape: (timesteps=694, channels=1)\n"
          ]
        }
      ],
      "source": [
        "# Reshape raw signals for deep learning\n",
        "print(\"\\nReshaping signals for deep learning...\")\n",
        "# Add channel dimension: (samples, timesteps) -> (samples, timesteps, channels)\n",
        "X_train_dl = X_train_raw.reshape(X_train_raw.shape[0], X_train_raw.shape[1], 1)\n",
        "X_val_dl = X_val_raw.reshape(X_val_raw.shape[0], X_val_raw.shape[1], 1)\n",
        "X_test_dl = X_test_raw.reshape(X_test_raw.shape[0], X_test_raw.shape[1], 1)\n",
        "\n",
        "print(f\"Reshaped for DL: {X_train_dl.shape}\")\n",
        "print(f\"Input shape: (timesteps={X_train_dl.shape[1]}, channels={X_train_dl.shape[2]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cLj7CHjjSqqH",
      "metadata": {
        "id": "cLj7CHjjSqqH"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRADITIONAL ML MODELS\n",
        "# ============================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7NA1FitaSsnj",
      "metadata": {
        "id": "7NA1FitaSsnj"
      },
      "outputs": [],
      "source": [
        "#Dictionary to store results\n",
        "ml_results = {}\n",
        "\n",
        "def train_and_evaluate_ml(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"Train and evaluate a traditional ML model\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Train\n",
        "    print(f\"  Training...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Probabilities (for ROC-AUC)\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
        "        y_val_proba = model.predict_proba(X_val)[:, 1]\n",
        "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_train_proba = y_train_pred\n",
        "        y_val_proba = y_val_pred\n",
        "        y_test_proba = y_test_pred\n",
        "\n",
        "    # Metrics\n",
        "    results = {\n",
        "        'model': model,\n",
        "        'train_acc': accuracy_score(y_train, y_train_pred),\n",
        "        'train_precision': precision_score(y_train, y_train_pred),\n",
        "        'train_recall': recall_score(y_train, y_train_pred),\n",
        "        'train_f1': f1_score(y_train, y_train_pred),\n",
        "        'train_auc': roc_auc_score(y_train, y_train_proba),\n",
        "        'val_acc': accuracy_score(y_val, y_val_pred),\n",
        "        'val_precision': precision_score(y_val, y_val_pred),\n",
        "        'val_recall': recall_score(y_val, y_val_pred),\n",
        "        'val_f1': f1_score(y_val, y_val_pred),\n",
        "        'val_auc': roc_auc_score(y_val, y_val_proba),\n",
        "        'test_acc': accuracy_score(y_test, y_test_pred),\n",
        "        'test_precision': precision_score(y_test, y_test_pred),\n",
        "        'test_recall': recall_score(y_test, y_test_pred),\n",
        "        'test_f1': f1_score(y_test, y_test_pred),\n",
        "        'test_auc': roc_auc_score(y_test, y_test_proba),\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'y_test_proba': y_test_proba\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n  Results:\")\n",
        "    print(f\"    Training   - Acc: {results['train_acc']:.4f}, F1: {results['train_f1']:.4f}, AUC: {results['train_auc']:.4f}\")\n",
        "    print(f\"    Validation - Acc: {results['val_acc']:.4f}, F1: {results['val_f1']:.4f}, AUC: {results['val_auc']:.4f}\")\n",
        "    print(f\"    Test       - Acc: {results['test_acc']:.4f}, F1: {results['test_f1']:.4f}, AUC: {results['test_auc']:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    joblib.dump(model, MODELS_DIR / f'{model_name.lower().replace(\" \", \"_\")}.pkl')\n",
        "    print(f\"Saved model\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "gXT-X_GUTKA_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXT-X_GUTKA_",
        "outputId": "d568baeb-8e97-4486-9a76-fc3560bcf080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. LOGISTIC REGRESSION (Baseline)\n",
            "\n",
            "============================================================\n",
            "Training Logistic Regression\n",
            "============================================================\n",
            "  Training...\n",
            "\n",
            "  Results:\n",
            "    Training   - Acc: 0.9855, F1: 0.9855, AUC: 0.9993\n",
            "    Validation - Acc: 0.9833, F1: 0.9833, AUC: 0.9990\n",
            "    Test       - Acc: 0.9817, F1: 0.9816, AUC: 0.9991\n",
            "Saved model\n"
          ]
        }
      ],
      "source": [
        "# 1. Logistic Regression (Baseline)\n",
        "print(\"\\n1. LOGISTIC REGRESSION (Baseline)\")\n",
        "lr = TraditionalMLModels.get_logistic_regression()\n",
        "ml_results['Logistic Regression'] = train_and_evaluate_ml(\n",
        "    lr, 'Logistic Regression',\n",
        "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "NzyMjwkuTcdx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzyMjwkuTcdx",
        "outputId": "238ba3f9-c5ca-4516-e78f-f50e3c987b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. RANDOM FOREST\n",
            "\n",
            "============================================================\n",
            "Training Random Forest\n",
            "============================================================\n",
            "  Training...\n",
            "\n",
            "  Results:\n",
            "    Training   - Acc: 1.0000, F1: 1.0000, AUC: 1.0000\n",
            "    Validation - Acc: 0.9925, F1: 0.9925, AUC: 0.9999\n",
            "    Test       - Acc: 0.9900, F1: 0.9900, AUC: 0.9998\n",
            "Saved model\n"
          ]
        }
      ],
      "source": [
        "# 2. Random Forest\n",
        "print(\"\\n2. RANDOM FOREST\")\n",
        "rf = TraditionalMLModels.get_random_forest(n_estimators=100)\n",
        "ml_results['Random Forest'] = train_and_evaluate_ml(\n",
        "    rf, 'Random Forest',\n",
        "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "QiSfqBeXTijB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiSfqBeXTijB",
        "outputId": "cdd851db-a778-4dd4-b6a8-68376c6c82a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. XGBOOST\n",
            "\n",
            "============================================================\n",
            "Training XGBoost\n",
            "============================================================\n",
            "  Training...\n",
            "\n",
            "  Results:\n",
            "    Training   - Acc: 1.0000, F1: 1.0000, AUC: 1.0000\n",
            "    Validation - Acc: 0.9958, F1: 0.9958, AUC: 0.9997\n",
            "    Test       - Acc: 0.9942, F1: 0.9942, AUC: 0.9998\n",
            "Saved model\n"
          ]
        }
      ],
      "source": [
        "# 3. XGBoost\n",
        "print(\"\\n3. XGBOOST\")\n",
        "xgb = TraditionalMLModels.get_xgboost(n_estimators=100)\n",
        "ml_results['XGBoost'] = train_and_evaluate_ml(\n",
        "    xgb, 'XGBoost',\n",
        "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8ZI4sthQTnXE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZI4sthQTnXE",
        "outputId": "0a47e770-600e-4904-c039-d878f0223a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. LIGHTGBM\n",
            "\n",
            "============================================================\n",
            "Training LightGBM\n",
            "============================================================\n",
            "  Training...\n",
            "\n",
            "  Results:\n",
            "    Training   - Acc: 1.0000, F1: 1.0000, AUC: 1.0000\n",
            "    Validation - Acc: 0.9967, F1: 0.9967, AUC: 0.9998\n",
            "    Test       - Acc: 0.9958, F1: 0.9958, AUC: 0.9999\n",
            "Saved model\n"
          ]
        }
      ],
      "source": [
        "# 4. LightGBM\n",
        "print(\"\\n4. LIGHTGBM\")\n",
        "lgb = TraditionalMLModels.get_lightgbm(n_estimators=100)\n",
        "ml_results['LightGBM'] = train_and_evaluate_ml(\n",
        "    lgb, 'LightGBM',\n",
        "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "z-oJFHttTusR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-oJFHttTusR",
        "outputId": "245348e0-e25a-47e9-de09-1af34e0c9f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5. SUPPORT VECTOR MACHINE\n",
            "\n",
            "============================================================\n",
            "Training SVM\n",
            "============================================================\n",
            "  Training...\n",
            "\n",
            "  Results:\n",
            "    Training   - Acc: 0.9955, F1: 0.9955, AUC: 0.9999\n",
            "    Validation - Acc: 0.9942, F1: 0.9942, AUC: 0.9999\n",
            "    Test       - Acc: 0.9958, F1: 0.9958, AUC: 0.9998\n",
            "Saved model\n"
          ]
        }
      ],
      "source": [
        "# 5. SVM\n",
        "print(\"\\n5. SUPPORT VECTOR MACHINE\")\n",
        "svm = TraditionalMLModels.get_svm(kernel='rbf')\n",
        "ml_results['SVM'] = train_and_evaluate_ml(\n",
        "    svm, 'SVM',\n",
        "    X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "G54EQYwoT1CR",
      "metadata": {
        "id": "G54EQYwoT1CR"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DEEP LEARNING MODELS\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6MHPxn4-T8zq",
      "metadata": {
        "id": "6MHPxn4-T8zq"
      },
      "outputs": [],
      "source": [
        "# Dictionary to store results\n",
        "dl_results = {}\n",
        "\n",
        "def train_and_evaluate_dl(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs=100):\n",
        "    \"\"\"Train and evaluate a deep learning model\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Model summary\n",
        "    print(f\"\\n  Model Architecture:\")\n",
        "    model.summary()\n",
        "    print(f\"  Total parameters: {model.count_params():,}\")\n",
        "\n",
        "    # Callbacks\n",
        "    model_callbacks = get_callbacks(model_name.lower().replace(\" \", \"_\"))\n",
        "\n",
        "    # Train\n",
        "    print(f\"\\n  Training...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=32,\n",
        "        callbacks=model_callbacks,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    y_train_proba = model.predict(X_train, verbose=0).flatten()\n",
        "    y_val_proba = model.predict(X_val, verbose=0).flatten()\n",
        "    y_test_proba = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "    y_train_pred = (y_train_proba > 0.5).astype(int)\n",
        "    y_val_pred = (y_val_proba > 0.5).astype(int)\n",
        "    y_test_pred = (y_test_proba > 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    results = {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'train_acc': accuracy_score(y_train, y_train_pred),\n",
        "        'train_precision': precision_score(y_train, y_train_pred),\n",
        "        'train_recall': recall_score(y_train, y_train_pred),\n",
        "        'train_f1': f1_score(y_train, y_train_pred),\n",
        "        'train_auc': roc_auc_score(y_train, y_train_proba),\n",
        "        'val_acc': accuracy_score(y_val, y_val_pred),\n",
        "        'val_precision': precision_score(y_val, y_val_pred),\n",
        "        'val_recall': recall_score(y_val, y_val_pred),\n",
        "        'val_f1': f1_score(y_val, y_val_pred),\n",
        "        'val_auc': roc_auc_score(y_val, y_val_proba),\n",
        "        'test_acc': accuracy_score(y_test, y_test_pred),\n",
        "        'test_precision': precision_score(y_test, y_test_pred),\n",
        "        'test_recall': recall_score(y_test, y_test_pred),\n",
        "        'test_f1': f1_score(y_test, y_test_pred),\n",
        "        'test_auc': roc_auc_score(y_test, y_test_proba),\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'y_test_proba': y_test_proba,\n",
        "        'epochs_trained': len(history.history['loss'])\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n  Results (after {results['epochs_trained']} epochs):\")\n",
        "    print(f\"    Training   - Acc: {results['train_acc']:.4f}, F1: {results['train_f1']:.4f}, AUC: {results['train_auc']:.4f}\")\n",
        "    print(f\"    Validation - Acc: {results['val_acc']:.4f}, F1: {results['val_f1']:.4f}, AUC: {results['val_auc']:.4f}\")\n",
        "    print(f\"    Test       - Acc: {results['test_acc']:.4f}, F1: {results['test_f1']:.4f}, AUC: {results['test_auc']:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    #model.save(MODELS_DIR / f'{model_name.lower().replace(\" \", \"_\")}.h5')\n",
        "    model.save(MODELS_DIR / f'{model_name.lower().replace(\" \", \"_\")}.keras')\n",
        "    print(f\"Saved model\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "nQVAa237UOml",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQVAa237UOml",
        "outputId": "243336de-aea4-48d2-b914-ac8f39b5f40b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Input shape for DL models: (694, 1)\n"
          ]
        }
      ],
      "source": [
        "# Input shape\n",
        "input_shape = (X_train_dl.shape[1], X_train_dl.shape[2])\n",
        "print(f\"\\nInput shape for DL models: {input_shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0deqgc7ZUTNs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0deqgc7ZUTNs",
        "outputId": "5071b52a-b262-4035-e837-a27eea026b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6. 1D CONVOLUTIONAL NEURAL NETWORK\n",
            "\n",
            "============================================================\n",
            "Training 1D CNN\n",
            "============================================================\n",
            "\n",
            "  Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"1D_CNN\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"1D_CNN\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">694</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">694</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">694</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ relu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">694</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,088</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ relu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bn_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ relu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">173</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_avg_pool                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m694\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m694\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bn_1 (\u001b[38;5;33mBatchNormalization\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m694\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ relu_1 (\u001b[38;5;33mActivation\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m694\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m41,088\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bn_2 (\u001b[38;5;33mBatchNormalization\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ relu_2 (\u001b[38;5;33mActivation\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │        \u001b[38;5;34m98,560\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bn_3 (\u001b[38;5;33mBatchNormalization\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ relu_3 (\u001b[38;5;33mActivation\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m173\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_avg_pool                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_dense (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183,169</span> (715.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m183,169\u001b[0m (715.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">182,273</span> (712.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m182,273\u001b[0m (712.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Total parameters: 183,169\n",
            "\n",
            "  Training...\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 1.01028, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 1.01028 to 0.48163, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3: val_loss improved from 0.48163 to 0.08343, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4: val_loss improved from 0.08343 to 0.04350, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5: val_loss did not improve from 0.04350\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.04350\n",
            "\n",
            "Epoch 7: val_loss did not improve from 0.04350\n",
            "\n",
            "Epoch 8: val_loss improved from 0.04350 to 0.03478, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9: val_loss did not improve from 0.03478\n",
            "\n",
            "Epoch 10: val_loss improved from 0.03478 to 0.02583, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 11: val_loss did not improve from 0.02583\n",
            "\n",
            "Epoch 12: val_loss did not improve from 0.02583\n",
            "\n",
            "Epoch 13: val_loss improved from 0.02583 to 0.02516, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 14: val_loss improved from 0.02516 to 0.01992, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 15: val_loss did not improve from 0.01992\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.01992\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.01992\n",
            "\n",
            "Epoch 18: val_loss did not improve from 0.01992\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.01992\n",
            "\n",
            "Epoch 20: val_loss improved from 0.01992 to 0.01467, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 21: val_loss did not improve from 0.01467\n",
            "\n",
            "Epoch 22: val_loss did not improve from 0.01467\n",
            "\n",
            "Epoch 23: val_loss improved from 0.01467 to 0.01041, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 24: val_loss did not improve from 0.01041\n",
            "\n",
            "Epoch 25: val_loss did not improve from 0.01041\n",
            "\n",
            "Epoch 26: val_loss did not improve from 0.01041\n",
            "\n",
            "Epoch 27: val_loss did not improve from 0.01041\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 28: val_loss did not improve from 0.01041\n",
            "\n",
            "Epoch 29: val_loss did not improve from 0.01041\n",
            "\n",
            "Epoch 30: val_loss improved from 0.01041 to 0.00693, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 31: val_loss improved from 0.00693 to 0.00623, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 32: val_loss did not improve from 0.00623\n",
            "\n",
            "Epoch 33: val_loss did not improve from 0.00623\n",
            "\n",
            "Epoch 34: val_loss improved from 0.00623 to 0.00576, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 35: val_loss did not improve from 0.00576\n",
            "\n",
            "Epoch 36: val_loss did not improve from 0.00576\n",
            "\n",
            "Epoch 37: val_loss did not improve from 0.00576\n",
            "\n",
            "Epoch 38: val_loss did not improve from 0.00576\n",
            "\n",
            "Epoch 39: val_loss improved from 0.00576 to 0.00459, saving model to ../results/models/1d_cnn_best.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 40: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 41: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 42: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 43: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 44: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 45: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 46: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 47: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 48: val_loss did not improve from 0.00459\n",
            "\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\n",
            "Epoch 49: val_loss did not improve from 0.00459\n",
            "Epoch 49: early stopping\n",
            "Restoring model weights from the end of the best epoch: 39.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Results (after 49 epochs):\n",
            "    Training   - Acc: 0.9973, F1: 0.9973, AUC: 1.0000\n",
            "    Validation - Acc: 0.9975, F1: 0.9975, AUC: 1.0000\n",
            "    Test       - Acc: 0.9967, F1: 0.9967, AUC: 1.0000\n",
            "Saved model\n"
          ]
        }
      ],
      "source": [
        "# 6. 1D CNN\n",
        "print(\"\\n6. 1D CONVOLUTIONAL NEURAL NETWORK\")\n",
        "cnn_model = DeepLearningModels.build_1d_cnn(input_shape)\n",
        "dl_results['1D CNN'] = train_and_evaluate_dl(\n",
        "    cnn_model, '1D CNN',\n",
        "    X_train_dl, y_train, X_val_dl, y_val, X_test_dl, y_test,\n",
        "    epochs=TRAINING_CONFIG['epochs']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "LwbVZTTTUX48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "LwbVZTTTUX48",
        "outputId": "b06a00a1-07dc-4e1c-fa80-67ade2faf5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "7. LSTM (Bidirectional)\n",
            "\n",
            "============================================================\n",
            "Training LSTM\n",
            "============================================================\n",
            "\n",
            "  Model Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"LSTM\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">694</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bilstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">694</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">133,120</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bilstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m694\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bilstm_1 (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m694\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m133,120\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bilstm_2 (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_dense (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">305,793</span> (1.17 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m305,793\u001b[0m (1.17 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">305,793</span> (1.17 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m305,793\u001b[0m (1.17 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Total parameters: 305,793\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=../results/models/lstm_best.h5",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7. LSTM (Bidirectional)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m DeepLearningModels\u001b[38;5;241m.\u001b[39mbuild_lstm(input_shape, bidirectional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m dl_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_and_evaluate_dl(\n\u001b[0;32m      5\u001b[0m     lstm_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     X_train_dl, y_train, X_val_dl, y_val, X_test_dl, y_test,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mTRAINING_CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m )\n",
            "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain_and_evaluate_dl\u001b[1;34m(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test, epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mcount_params()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model_callbacks \u001b[38;5;241m=\u001b[39m get_callbacks(model_name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\sridi\\Documents\\NE\\Fall2024\\IE6400\\Group projects\\EEG-Classification\\notebooks\\../src\\models.py:510\u001b[0m, in \u001b[0;36mget_callbacks\u001b[1;34m(model_name, patience, min_delta)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_callbacks\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    476\u001b[0m                  patience: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    477\u001b[0m                  min_delta: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    478\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m    Get training callbacks\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m        List of Keras callbacks\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     callback_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    496\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m    497\u001b[0m             monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    498\u001b[0m             patience\u001b[38;5;241m=\u001b[39mpatience,\n\u001b[0;32m    499\u001b[0m             min_delta\u001b[38;5;241m=\u001b[39mmin_delta,\n\u001b[0;32m    500\u001b[0m             restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    501\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    502\u001b[0m         ),\n\u001b[0;32m    503\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[0;32m    504\u001b[0m             monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    505\u001b[0m             factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m    506\u001b[0m             patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m    507\u001b[0m             min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m,\n\u001b[0;32m    508\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    509\u001b[0m         ),\n\u001b[1;32m--> 510\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m    511\u001b[0m             filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_best.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    512\u001b[0m             monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    513\u001b[0m             save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    515\u001b[0m         )\n\u001b[0;32m    516\u001b[0m     ]\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callback_list\n",
            "File \u001b[1;32mc:\\Users\\sridi\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n",
            "\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=../results/models/lstm_best.h5"
          ]
        }
      ],
      "source": [
        "# 7. LSTM\n",
        "print(\"\\n7. LSTM (Bidirectional)\")\n",
        "lstm_model = DeepLearningModels.build_lstm(input_shape, bidirectional=True)\n",
        "dl_results['LSTM'] = train_and_evaluate_dl(\n",
        "    lstm_model, 'LSTM',\n",
        "    X_train_dl, y_train, X_val_dl, y_val, X_test_dl, y_test,\n",
        "    epochs=TRAINING_CONFIG['epochs']\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
